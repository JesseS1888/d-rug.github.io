---
title: "Basics of Linear Modeling"
author: "Evan Batzer"
date: "October 17, 2019"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2);library(gridExtra); library(dplyr)
cars_subset <- mtcars %>% select(mpg, disp, hp, wt, drat)
```

<style type="text/css">
code.r{ /* Code block */
    font-size: 22px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 22px;
}
</style>

## What is a *linear model*?

A statistical model where a *response* is estimated as a *linear function* of predictors.

**Linear operations include:** 

- Adding vectors (+)
- Multiplying vectors by a constant (*)

**Simplest case: **

- $y = mx + b$ (grade school)
- $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ (grown-up)

An observation (i) has a mean value that is estimated by an intercept coefficient ($\beta_0$) and a slope coefficient ($\beta_1$) multiplied by a predictor ($X$) with some random error ($\epsilon_i$)

## Variations

**Case 1:**
Classic linear model | lm()

- Observations (Y's) come from a normal distribution with constant variance
- Predictors are fixed (not random)

**Case 2:**
Random / Mixed Effects Model | lmer(), lme()

- Observations (Y's) come from a normal distribution with constant variance
- Predictors can be either fixed OR random

**Case 3:**
Generalized Linear Model | glm(), glmer()

- Observations (Y's) can come from a distribution whose *mean* is estimated by a linear function of predictors.
- Poisson regression, logistic regression

## 

Despite differences in linear model types, **analysis steps** and **model formulation** are remarkably consistent

Here, we'll be using data from the *mtcars* (Motor Trends Car Road Tests) dataset to explore the basics of linear modeling.

```{r, echo = TRUE}
cars_subset <- mtcars %>% select(mpg, disp, hp, wt, drat)
```

## Basic roadmap

1. Define question
2. Collect and clean data
3. Exploring predictors
4. Fitting model objects
5. Model diagnostics
6. Inference and visualization

## 1. Define your question 

Have a defined question (or hypothesis) when starting analysis can help guide your modeling approach.

- What predictor variables are of interest?
- Am I looking to test the fit of a particular model? 
- Do I want to compare many potential models?
- Am I interested in finding the combination of variables that best predicts a response?

As an example:

**What variables best predict the fuel efficiency (mpg) of a car?**

*Keep this question in mind at all times going forward*

## 2. Collect and clean data

Will not cover this in much detail today, but know that:

**Data must be in a consistent for many modeling packages**

- Rows correspond to unique observations 
- Columns correspond to variables (dependent and independent)

```{r}
head(cars_subset)
```

## 3. Exploring predictors

**How are my predictors distributed?**

- Highly skewed predictors can be problematic -- outlying points and non-linear relationships
- Highly correlated predictors will increase parameter variance

Inspect predictors with hist() or ggplot2::geom_histogram(), transform if needed

```{r, fig.height=3}
p1 <- ggplot(cars_subset)+ 
  geom_histogram(aes(x = hp), bins = 15,
                 fill = "coral", color = "black") +
  ggtitle("Histogram of Horsepower")

p2 <- ggplot(cars_subset)+ 
  geom_histogram(aes(x = log(hp)), bins = 15,
                 fill = "coral", color = "black") +
   ggtitle("Histogram of Log(Horsepower)")

grid.arrange(p1, p2, nrow = 1)
```

## pairs() shows bivariate scatterplots

```{r, echo = TRUE}
pairs(cars_subset %>% select(disp, hp, wt, drat), cex = 2)
```

## corrplot() returns pretty correlation diagrams ###

```{r, echo = TRUE, fig.height = 4}
corrplot::corrplot.mixed(cor(cars_subset %>% select(disp, hp, wt, drat)))
```

## Variance inflation factor (VIF)

**Variance inflation factor** (VIF) can be used to determine how highly correlated predictors affect one another in regression.

Simply, if two predictors are highly correlated, it can be hard to determine which variable has an effect on a relationship. High VIF values make large uncertainty in the effects of a predictor.

```{r, echo = TRUE}
diag(solve(cor(cars_subset %>% select(disp, hp, wt, drat))))
```

Here, a car's horsepower, weight, and displacement are all correlated with one another. If gas mileage is poor, which one of these is the culprit? 

## 4. Fitting Model Objects

**Function calls in all major model-fitting packages take the form:**
```{r}
cat("lm(Response ~ Predictors)")
```

By design, the intercept coefficient, $\beta_0$ is assumed to be added.

**In a practical example:**
```{r}
cat("lm(mpg ~ hp, data = cars_subset)")
```

Translates to:
$$Y = \beta_0 + \beta_1 * Horsepower + \epsilon$$

## Model Notation

**Removing the intercept**

You'll have to specify this manually with "0 +" or "-1"
```{r, echo = TRUE}
coef(lm(mpg ~ hp, data = cars_subset))
coef(lm(mpg ~ 0 + hp, data = cars_subset))
```

\br

## Model Notation (Continued) 

**Adding terms**

The "+" operator

```{r, echo = TRUE}
coef(lm(mpg ~ hp + wt + disp + drat, data = cars_subset))
```

## Model Notation (Continued) 

**Interaction terms**

Interaction terms are designated through ":"

```{r, echo = TRUE}
coef(lm(mpg ~ hp:wt, data = cars_subset))
```

Including the "*" operator returns first-order terms and their interactions

```{r, echo = TRUE}
coef(lm(mpg ~ hp*wt, data = cars_subset))
```

## Model Notation (Continued) 

**Removing Predictors**

Sometimes, you may want to remove a predictor from a model formula with "-"

```{r, echo = TRUE}
coef(lm(mpg ~ hp*wt - hp:wt, data = cars_subset))
```

## Model Notation (Continued) 

**Including all predictors with "."**

```{r, echo = TRUE}
coef(lm(mpg ~ ., data = cars_subset))
```

*Note:* Previous operations still apply!

```{r, echo = TRUE}
coef(lm(mpg ~ . * ., data = cars_subset))
```

## Model Notation (Continued) 

**Inhibiting interpretation**

If you want to create new variables that are defined by predictors multiplied or added together, operations can be inhibited with "I()"

```{r, echo = TRUE}
coef(lm(mpg ~ I(hp * wt), data = cars_subset))
```

## Model Notation (Continued) 

**Exponential Terms**

"I()" is important for polynomials. Exponential terms are assigned through "^". **However, without I() this operation will produce first-and second-order terms!**

```{r, echo = TRUE}
coef(lm(mpg ~ hp + I(hp^2), data = cars_subset))
```

```{r, echo = TRUE}
coef(lm(mpg ~ hp + hp^2, data = cars_subset))
```

## Model Notation (Continued) 

**"^" with multiple coefficients:**

```{r, echo = TRUE}
coef(lm(mpg ~ (hp + wt)^2, data = cars_subset))
```

## Quiz!

My data frame contains 5 variables: mpg, hp, wt, disp, drat

```{r, size=15}
cat("lm(mpg ~ hp*wt*disp*drat - hp:wt:disp:drat, data = cars_subset)")
```
\br

```{r, size=15}
cat("lm(mpg ~ .*.*. , data = cars_subset)")
```

\br
```{r, size=15}
cat("lm(mpg ~ (hp+wt+disp+drat)^3, data = cars_subset)")
```

What coefficients do these different models produce?

## Answer:

<style type="text/css">
code.r{ /* Code block */
    font-size: 18px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 18px;
}
</style>

Trick question -- they're all the same!

```{r, echo = TRUE}
coef(lm(mpg ~ disp * hp * wt * drat - hp:wt:disp:drat, data = cars_subset))
```

## Answer:

Trick question -- they're all the same!

```{r, echo = TRUE}
coef(lm(mpg ~ .*.*. , data = cars_subset))
```

## Answer:

Trick question -- they're all the same!

```{r, echo = TRUE}
coef(lm(mpg ~ (disp + hp + wt + drat)^3, data = cars_subset))
```

All first, second, and third order terms

## A guide to your fitted model object

**Fitting a new model**
```{r, echo = TRUE}
mymodel <- lm(mpg ~ hp, data = cars_subset)
```

\br

**What formula did I run?**

```{r, echo = TRUE}
mymodel$call
```

```{r, echo = TRUE}
formula(mymodel)
```
## Your fitted model object (continued)

**summary() provides a general overview of your model**

```{r}
summary(mymodel)
```

## Your fitted model object (continued)

**anova() provides the sum-of-squares decomposition**

```{r}
anova(mymodel)
```

## Your fitted model object (continued)

**Add new predictors to an existing model with update()**

```{r, echo = TRUE}
update(mymodel, formula = ~ . + disp)
```

## 5. Regression Diagnostics

[There are many useful links to interpret diagnostic plots](https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html). Here, I'll focus on diagnostics for a simple (normal) linear model.

**Plot 1 -- Residuals vs. Fitted**

Shows the relationship between estimated response (Fitted values) and error (Residuals). Ideally, these fall along a straight line with even spread.

```{r, fig.height=3.5}
plot(mymodel, which = 1)
```

## Regression Diagnostics

**Plot 2 -- Quantile-Quantile (QQ) Plot**

[The QQ plot demonstrates how closely our observed residuals resemble a normal distribution.](https://data.library.virginia.edu/understanding-q-q-plots/) Values should fall along the 1:1 line.

```{r, fig.height=3.5}
plot(mymodel, which = 2)
```

## Regression Diagnostics

**Plot 3 -- Scale-Location**

Another way to see non-random patterns in residuals. Trends with sloped lines show greater variance as predicted values increase (similar to a funnel-shaped residuals vs. fitted plot)

```{r, fig.height=3.5}
plot(mymodel, which = 3)
```

## Regression Diagnostics

**Plot 4 -- Cook's Distance**

Useful to check for outliers -- high values indicate observations that have a large impact on our fitted model.

```{r, fig.height=3.5}
plot(mymodel, which = 4)
```

## 6. Inference and Visualization

**Prediction:**

The predict() function can be used to estimate responses given a model. By specifying a new dataframe, custom predictions with mean response confidence intervals ("confidence") or a new obseration intervals ("prediction") can be generated.

```{r, echo = TRUE, eval = FALSE}
# Estimation of means
predict(mymodel, newdata = data.frame(disp = seq(50, 200, by = 25)), 
        interval = "confidence")

# Estimation of a new observation
predict(mymodel, newdata = data.frame(disp = seq(50, 200, by = 25)), 
        interval = "prediction")
```


## Inference and Visualization

The **[emmeans](https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html)** package can be used to provide estimated responses and contrasts between different values. Particularly useful when you have categorical predictors.

```{r, echo=TRUE}
# New model with automatic/manual as a variable
newmodel <- lm(mpg ~ as.factor(am), data = mtcars)

model_means <- emmeans::emmeans(newmodel, "am")
pairs(model_means)
```

## Inference and Visualization

The [sjPlot package](http://www.strengejacke.de/sjPlot/) provides some clean visualizations of parameter effects that produces a ggPlot object.

```{r, echo = TRUE, eval = FALSE}
multiple_model <- lm(mpg ~ ., data = cars_subset)
sjPlot::plot_model(multiple_model, type = "pred")
```
```{r, fig.height=3.5}
multiple_model <- lm(mpg ~ ., data = cars_subset)
plots <- sjPlot::plot_model(multiple_model, type = "pred")
grid.arrange(plots$disp, plots$hp, plots$wt, plots$drat)
```


## Inference and Visualization

As well as coefficient plots with associated confidence intervals.

```{r, echo = TRUE, eval = FALSE}
multiple_model <- lm(mpg ~ ., data = cars_subset)
sjPlot::plot_model(multiple_model, type = "std")
```

```{r, fig.height=3.5}
multiple_model <- lm(mpg ~ ., data = cars_subset)
sjPlot::plot_model(multiple_model, type = "std")
```

## Other resources

There are tons of free resources available online to help with all types of linear modeling. 

Some examples:

[r-statistics.co](http://r-statistics.co/Linear-Regression.html)

[Penn State's online statistics course materials](https://newonlinecourses.science.psu.edu/stat555/node/72/)

[Ben Bolker on Generalized/Linear Mixed Effects Models](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)

## Contact

Questions? Get in touch at:

**Email:** ebatzer@ucdavis.edu

**Twitter:** @[eebatzer](https://www.twitter.com/eebatzer)

**Website:** [www.evanbatzer.com](https://www.evanbatzer.com)
